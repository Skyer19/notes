## 评估设置

- 机器学习的最终目标是创建可以推广到未知数据的模型和算法
- 为了确保评估有意义，基本要求是使用保留的测试数据集（绝对不要使用测试数据集来训练模型）

## 超参数调整

- Naive approach
	- 在训练数据集上尝试不同的值，根据训练数据集上的准确率选择最佳值，然后在测试数据集上进行评估
	- 问题：通常效果不是很好

- Right approach
	- 将数据集分为 3 部分：训练/验证/测试，常见分割为 60%/20%/20% 和 80%/10%/10%
	- 在训练数据集上尝试不同的超参数值，根据验证数据集上的准确率选择最佳值，然后在测试数据集上执行最终评估
	- 优势：您的超参数选择考虑了模型的泛化方式，最终评估仍然仅使用以前未见过的数据。


The final evaluation:
- use the model trained on the training dataset
- or combine the training and validation datasets and train a new model (with the same parameters)

## 交叉验证

用处：当数据集不大，无法满足有足够多的数据用于训练/验证/测试


- 将数据集分成 k 个（通常为 10 个）相等的fold。使用 k-1 个fold进行训练+验证，使用一个fold进行测试。
- 迭代 k 次，每次测试数据的不同部分。
- 然后可以对所有 k 个保留测试集的性能进行平均  $error = \frac{1}{N}\sum_{i=1}^{N}e_{i}$

算法A：

- 在每次迭代中，使用1个折用于测试，1个折用于验证，剩余的k-2个折用于训练。
- 当我们使用多个训练集时，我们评估的是算法，而不是特定的模型。
- 关键点：每个折中找到一组不同的最优参数。

算法B：

- 在每次交叉验证步骤中，分离1个折用于测试。
- 对剩下的k-1个折进行 **内部交叉验证** ，以找到最优的超参数。
- 关键点：每个折仍然会有不同的超参数，只是基于更多数据来选择。

在完成交叉验证后，使用最好表现的超参数重新在数据集上进行训练

## 评估指标

### 分类任务

#### 两个标签
Eg: We have two classes (class A: positive and class B: negative) Take Class A as example in the formulas.

- Confusion matrix (混淆矩阵)
<div class="center-table" markdown>

|             | Class A Prediction    | Class B Prediction |
|----------------|-------|------------|
|Class A Actual |TP: True Positive|FN: True Negative|
|Class B Actual|FP: False Positive|FN: False Negative|

</div>
 - 准确率：正确分类的示例数除以示例总数；分类错误=1-准确率

$$Accuracy = \frac{TP+TN}{TP+TN+FP+FN}$$

- 精度：正确分类的 A 类示例数除以预测的 A 类示例总数

$$Precision = \frac{TP}{TP+FP}$$

- 召回率：正确分类的 A 类样本数除以 A 类样本总数


	1. High recall, low precision: Most of the positive examples are correctly recognised (low FN) but there are many false positives(high FP).
	2. Low recall, high precision: Miss a lot of positive examples (high FN) but those predict as positive are really positive (low FP).

$$Recall = \frac{TP}{TP+FN}$$

$$F1 = \frac{2 \times precision \times recall}{precision+recall}$$

$$F_{β} = (1+β^{2})\frac{2 \times precision \times recall}{(β^{2} \times precision)+recall}$$

#### 多类别

- 多类别混淆矩阵
	- 将一个类别定义为正类，将其他类别定义为负类。
	- 然后用完全相同的方式计算指标。
		- 准确率 = 正确分类的示例数除以示例总数。
		- 针对每个类别分别计算准确率、召回率和 F1。

- Micro - vs macro-averaging
	- 宏观平均：计算每个类别的指标并计算平均值
		- e.g., Macro-averaged recall = mean(Recall for class 1, Recall for class 2...)
	- 微观平均：在项目级别取平均值
		- e.g., Micro-averaged recall = TP for every class/(TP for every class + FN for every class)

### 回归任务

Mean squared error (MSE): $\frac{1}{N}(Y_{i}-\hat{Y_{i}})^{2}$

Root mean squared error (RMSE): $\sqrt{MSE}$

## 数据分布不均衡

总结：

- 准确率可能会产生误导，它只是遵循了大多数类别的表现。
- 宏平均召回率也可以帮助检测一个类别是否完全被错误分类，但它并没有给我们提供有关 FP 的信息
- F1 很有用，但也会受到类别不平衡问题的影响。我们不确定低分是由于一个类别被错误分类还是类别不平衡造成的。

方法：

- 对多数类进行下采样。例如，随机选择与少数类相同数量的示例
- 或对少数类进行上采样（创建并添加重复项，直到两个类的大小相同）

将有助于平衡类别，但结果不会反映该模型的泛化能力 --- 实际数据仍然来自不平衡的分布

## 过拟合和不拟合

- 过拟合：在训练数据上表现良好，但对其他数据的泛化能力较差。
	- 原因：1）模型复杂 2）训练集不具代表性 3）训练时间长
	- 解决办法：1）使用正确的复杂度（使用验证集来决定）2）更多数据3）提前停止
- 欠拟合：在训练数据上表现不佳，但对其他数据的泛化能力较差。


解决神经网络中的过拟合问题：

- 网络容量：通过减少神经元/层的数量来降低其容量。 
- 提前停止：使用验证数据来选择何时停止 
- 正则化：添加一些信息或约束来阻止模型过度拟合。（L1（鼓励特征稀疏，模型将只保留最重要的特征），L2（将权重推向零，权重越大，受到的惩罚越多）：

## 置信区间

## 统计显著性检验

